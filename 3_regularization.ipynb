{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "L2 regularization을 logistic model과 neural network nodel에 도입 해라. (hint source from [Arn-O](https://github.com/Arn-O/udacity-deep-learning \"Arn-O's github\"))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Model(SGD) with L2 Regularization (Regularizing with beta : not hardcoding)\n",
    "\n",
    "beta의 변화에 따른 test accuracy를 plot하여, 최적의 beta를 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 #작게..\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed / 입력.\n",
    "  # at run time with a training minibatch. / training data는 minibatch를 실시간으로 먹이기 위해 placeholder를 쓴다.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32) # beta will be fed\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "\n",
    "  # Loss function using L2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  loss = tf.reduce_mean(loss + beta_regul*tf.nn.l2_loss(weights))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.420685\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 500: 2.846879\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 1000: 1.738102\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 1500: 1.301310\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2000: 0.987479\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2500: 0.826031\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 3000: 0.858450\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.4%\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "# Test with beta = 0.0001\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized. / 랜덤화된 training data안에 offset을 만든다.\n",
    "    # Note: we could use better randomization across epochs. / (시간?!) 랜덤을 더 잘 사용.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # offset은 minibatch를 만들기 위해서..\n",
    "    # Generate a minibatch. / minibatch를 만든다.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :] \n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch. / 사전 준비 세션을 말하는 어디에서 미니배치를 먹는지\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed, / 사전의 포인트는 먹히기 위해 그래프의 플홀 노드이다.\n",
    "    # and the value is the numpy array to feed to it. / 그리고 그 값은 넘피 배열이다.\n",
    "    # tensorflow의 docs에서 session.run()에 대한 부분을 보면, feed_dict은 다음과 같이 설명되어 있다.\n",
    "    # \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta의 최적값을 찾기 위해 plot을 그려본다.(beta is a new meta parameter. We have to tune it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEQCAYAAAC5oaP8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f79fc352050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1-layer network model with L2 regulation(Regularizing with beta : not hardcoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_node = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed / 입력.\n",
    "    # at run time with a training minibatch. / training data는 minibatch를 실시간으로 먹이기 위해 placeholder를 쓴다.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_node]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_node]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_node, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    def forward_prop(inp):\n",
    "        hidden_layer1 = tf.nn.relu(tf.matmul(inp, weights1)+biases1)\n",
    "        return tf.matmul(hidden_layer1, weights2) + biases2\n",
    "    \n",
    "    # Training computation.\n",
    "    \n",
    "    logits = forward_prop(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    loss = tf.reduce_mean(loss + beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)))\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(forward_prop(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(forward_prop(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 662.818176\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 35.2%\n",
      "Minibatch loss at step 500: 195.308762\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1000: 118.649803\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1500: 68.517281\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 2000: 41.434418\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 2500: 25.169493\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 3000: 15.492086\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.2%\n",
      "Test accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "# Test with beta = 0.0001\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized. / 랜덤화된 training data안에 offset을 만든다.\n",
    "    # Note: we could use better randomization across epochs. / (시간?!) 랜덤을 더 잘 사용.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # offset은 minibatch를 만들기 위해서..\n",
    "    # Generate a minibatch. / minibatch를 만든다.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :] \n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch. / 사전 준비 세션을 말하는 어디에서 미니배치를 먹는지\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed, / 사전의 포인트는 먹히기 위해 그래프의 플홀 노드이다.\n",
    "    # and the value is the numpy array to feed to it. / 그리고 그 값은 넘피 배열이다.\n",
    "    # tensorflow의 docs에서 session.run()에 대한 부분을 보면, feed_dict은 다음과 같이 설명되어 있다.\n",
    "    # \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see test accuracy with beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEQCAYAAABLMTQcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfOj/AUpt1fwu1R9EAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f79ae719250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "극도로 overfitting된 경우를 한번 해보자. training data를 몇몇 batch에서 제한시켜보자. 어떻게 될까?\n",
    "(1-layer network applied to L2 regulation(beta : 0.0001))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_node = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed / 입력.\n",
    "    # at run time with a training minibatch. / training data는 minibatch를 실시간으로 먹이기 위해 placeholder를 쓴다.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_node]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_node]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_node, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    def forward_prop(inp):\n",
    "        hidden_layer1 = tf.nn.relu(tf.matmul(inp, weights1)+biases1)\n",
    "        return tf.matmul(hidden_layer1, weights2) + biases2\n",
    "    \n",
    "    # Training computation.\n",
    "    \n",
    "    logits = forward_prop(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    loss = tf.reduce_mean(loss + beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)))\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(forward_prop(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(forward_prop(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 646.448792\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 34.7%\n",
      "Minibatch loss at step 10: 312.049591\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 20: 308.943909\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 30: 305.869232\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 40: 302.825287\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 50: 299.811127\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 60: 296.827301\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 70: 293.873077\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 80: 290.948242\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 90: 288.052765\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 100: 285.185699\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Test accuracy: 70.2%\n"
     ]
    }
   ],
   "source": [
    "# Test with beta = 0.0001\n",
    "num_steps = 101\n",
    "num_batches = 10\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized. / 랜덤화된 training data안에 offset을 만든다.\n",
    "    # Note: we could use better randomization across epochs. / (시간?!) 랜덤을 더 잘 사용.\n",
    "    offset = (step * batch_size) % num_batches # offset은 minibatch를 만들기 위해서..\n",
    "    # Generate a minibatch. / minibatch를 만든다.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :] \n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch. / 사전 준비 세션을 말하는 어디에서 미니배치를 먹는지\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed, / 사전의 포인트는 먹히기 위해 그래프의 플홀 노드이다.\n",
    "    # and the value is the numpy array to feed to it. / 그리고 그 값은 넘피 배열이다.\n",
    "    # tensorflow의 docs에서 session.run()에 대한 부분을 보면, feed_dict은 다음과 같이 설명되어 있다.\n",
    "    # \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minibatch에 대한 학습은 100%에 도달하였으나, 다른 항목들을 보면 학습 결과가 형편없음을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "hidden layer에 Dropout을 도입한다. 기억할 사항 : dropout은 오로지 training할 때만 적용(evaluation X). TensorFlow는 `nn.dropout()`을 제공해준다. 다시 한번 말하지만, 오직 training할 때만 이 함수를 삽입해라.\n",
    "\n",
    "질문 : 극도로 overfitting되는 경우, 어떤 일이 벌어지는지?\n",
    "\n",
    "(1-layer network with L2 regulation. restrain the number of batch like above. Just apply dropout, and compare with above)\n",
    "(L2 regulation 포함에 dropout 적용. 바로 위 problem 2와 동일한 조건에서 실행하여 결과를 비교해보자.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_node = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed / 입력.\n",
    "    # at run time with a training minibatch. / training data는 minibatch를 실시간으로 먹이기 위해 placeholder를 쓴다.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_node]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_node]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_node, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    def forward_prop(inp):\n",
    "        hidden_layer1 = tf.nn.relu(tf.matmul(inp, weights1)+biases1)\n",
    "        return tf.matmul(hidden_layer1, weights2) + biases2\n",
    "    \n",
    "    # Training computation.\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1)+biases1)\n",
    "    dropout = tf.nn.dropout(hidden_layer1, 0.5) # apply dropout on hidden layer\n",
    "    logits = tf.matmul(dropout, weights2) + biases2 # logits would be calculated with dropout instead of hidden_layer1\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    loss = tf.reduce_mean(loss + beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)))\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(forward_prop(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(forward_prop(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 783.694702\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 36.3%\n",
      "Minibatch loss at step 10: 313.633545\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 20: 309.905975\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 30: 306.150299\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 40: 303.530029\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 50: 300.118134\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 60: 298.202759\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 72.0%\n",
      "Minibatch loss at step 70: 294.181915\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch loss at step 80: 291.255890\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 90: 288.364044\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch loss at step 100: 285.500885\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.2%\n",
      "Test accuracy: 77.6%\n"
     ]
    }
   ],
   "source": [
    "# Test with beta = 0.0001\n",
    "num_steps = 101\n",
    "num_batches = 10\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized. / 랜덤화된 training data안에 offset을 만든다.\n",
    "    # Note: we could use better randomization across epochs. / (시간?!) 랜덤을 더 잘 사용.\n",
    "    offset = (step * batch_size) % num_batches # offset은 minibatch를 만들기 위해서..\n",
    "    # Generate a minibatch. / minibatch를 만든다.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :] \n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch. / 사전 준비 세션을 말하는 어디에서 미니배치를 먹는지\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed, / 사전의 포인트는 먹히기 위해 그래프의 플홀 노드이다.\n",
    "    # and the value is the numpy array to feed to it. / 그리고 그 값은 넘피 배열이다.\n",
    "    # tensorflow의 docs에서 session.run()에 대한 부분을 보면, feed_dict은 다음과 같이 설명되어 있다.\n",
    "    # \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여전히 minibatch에 대해선 overfitting된 경향을 보이나, problem 2와는 다르게 99%로 감소하는 구간도 있고, 100%에 달성하기까지의 단계도 더 오래 걸린다. 무엇보다 validation, test accuracy가 향상되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "multi-layer model을 이용해서 최고의 퍼포먼스를 만들어봐라. 지금까지는 97.1%의 test accuracy가 가장 높은 퍼포먼스이다.\n",
    "\n",
    "multiple layer를 추가하는 것이 방법이 될 수 있다.\n",
    "\n",
    "또 다른 방법으로는 learning rate decay를 적용시키는 것이다.\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Trial\n",
    "\n",
    "2-layer network with L2 generalization and dropout(only first layer). And also use learning rate decay(decay every 1000 steps with a base of 0.90)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_node1 = 1024\n",
    "hidden_node2 = 512\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed / 입력.\n",
    "    # at run time with a training minibatch. / training data는 minibatch를 실시간으로 먹이기 위해 placeholder를 쓴다.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [image_size * image_size, hidden_node1], stddev=np.sqrt(2.0/(image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_node1]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_node1, hidden_node2], stddev=np.sqrt(2.0/hidden_node1)))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_node2]))\n",
    "    \n",
    "    weights3 = tf.Variable(tf.truncated_normal([hidden_node2, num_labels], stddev=np.sqrt(2.0/hidden_node2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    def forward_prop(inp):\n",
    "        h_layer1 = tf.nn.relu(tf.matmul(inp, weights1)+biases1)\n",
    "        h_layer2 = tf.nn.relu(tf.matmul(h_layer1, weights2)+biases2)\n",
    "        return tf.matmul(h_layer2, weights3) + biases3\n",
    "    \n",
    "    # Training computation.\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1)+biases1)\n",
    "    dropout = tf.nn.dropout(hidden_layer1, 0.5) # apply dropout on hidden layer\n",
    "    hidden_layer2 = tf.nn.relu(tf.matmul(dropout, weights2)+biases2)\n",
    "    logits = tf.matmul(hidden_layer2, weights3) + biases3 # logits would be calculated with dropout instead of hidden_layer1\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    loss = tf.reduce_mean(loss + beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)+tf.nn.l2_loss(weights3)))\n",
    "  \n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 4000, 0.90, staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(forward_prop(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(forward_prop(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of steps = 3001, beta = 0.00011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.635016\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 23.1%\n",
      "Minibatch loss at step 500: 0.697492\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 1000: 0.723921\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 1500: 0.710466\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2000: 0.575264\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 2500: 0.527446\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 3000: 0.673576\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 3500: 0.571718\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 4000: 0.546977\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 4500: 0.381349\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 5000: 0.454287\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 5500: 0.482566\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 6000: 0.565564\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 6500: 0.472728\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 7000: 0.440844\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 7500: 0.492662\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 8000: 0.719435\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 8500: 0.593376\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 9000: 0.421445\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.2%\n",
      "Test accuracy: 95.0%\n"
     ]
    }
   ],
   "source": [
    "# Test with beta = 0.00011\n",
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized. / 랜덤화된 training data안에 offset을 만든다.\n",
    "    # Note: we could use better randomization across epochs. / (시간?!) 랜덤을 더 잘 사용.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # offset은 minibatch를 만들기 위해서..\n",
    "    # Generate a minibatch. / minibatch를 만든다.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :] \n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch. / 사전 준비 세션을 말하는 어디에서 미니배치를 먹는지\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed, / 사전의 포인트는 먹히기 위해 그래프의 플홀 노드이다.\n",
    "    # and the value is the numpy array to feed to it. / 그리고 그 값은 넘피 배열이다.\n",
    "    # tensorflow의 docs에서 session.run()에 대한 부분을 보면, feed_dict은 다음과 같이 설명되어 있다.\n",
    "    # \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 0.00011}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
